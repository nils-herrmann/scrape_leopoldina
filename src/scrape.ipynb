{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9b6ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import json\n",
    "import logging\n",
    "from time import sleep\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "from urllib.parse import urljoin, unquote\n",
    "\n",
    "from constants import ALL_COLUMNS, IRRELEVANT_COLUMNS, RENAME_COLUMNS, SHOULD_BE_NOTEMPTY_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa862b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[logging.FileHandler(\"leopoldina_scraper.log\", encoding='utf-8')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75005393",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://de.wikipedia.org\"\n",
    "BASE_TITLE = \"Liste_der_Mitglieder_der_Deutschen_Akademie_der_Naturforscher_Leopoldina\"\n",
    "API_URL = \"https://de.wikipedia.org/w/api.php\"\n",
    "YEARS = range(1885, 1951)\n",
    "\n",
    "all_tables = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5bfdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Starting to scrape Leopoldina members data\")\n",
    "for year in tqdm(YEARS):\n",
    "    title = f'{BASE_TITLE}/{year}'\n",
    "    logging.info(f\"{year}: Fetching page via Wikipedia API...\")\n",
    "    \n",
    "    params = {\n",
    "        \"action\": \"parse\",\n",
    "        \"page\": title,\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"text\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(API_URL, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        if \"error\" in data:\n",
    "            logging.warning(f\"\\t{year}: Page not found (API returned error).\")\n",
    "            continue\n",
    "        \n",
    "        html_content = data[\"parse\"][\"text\"][\"*\"]\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        tables = soup.select(\"div.mw-parser-output > table\")\n",
    "\n",
    "        if tables:\n",
    "            if len(tables) > 1:\n",
    "                logging.warning(f\"\\t{year}: More than one table found, using the first one.\")\n",
    "            \n",
    "            # HTML table to DataFrame\n",
    "            table = tables[0]\n",
    "            table_html = str(table)\n",
    "            df = pd.read_html(StringIO(table_html))[0]\n",
    "\n",
    "            # Extract links for the \"Name\" column (second column)\n",
    "            name_links = []\n",
    "            for row in table.select(\"tr\")[1:]: # Skip header row\n",
    "                cells = row.find_all(\"td\")\n",
    "                if len(cells) > 1:  # Ensure the second column exists\n",
    "                    name_cell = cells[1]  # Second column\n",
    "                    if name_cell.find(\"a\"):\n",
    "                        link = name_cell.find(\"a\").get(\"href\")\n",
    "                        if link and \"redlink=1\" not in link: # Ignore redlinks (Missing pages)\n",
    "                            full_link = urljoin(BASE_URL, link)\n",
    "                            decoded_link = unquote(full_link) # Decode URL\n",
    "                            name_links.append(full_link)\n",
    "                        else:\n",
    "                            name_links.append(None)\n",
    "                    else:\n",
    "                        name_links.append(None)\n",
    "                else:\n",
    "                    name_links.append(None)\n",
    "\n",
    "            # Add the links as a new column in the DataFrame\n",
    "            df[\"Link\"] = name_links\n",
    "\n",
    "            all_tables[year] = df\n",
    "            logging.info(f\"\\t{year}: Table found with {len(df)} rows.\")\n",
    "        else:\n",
    "            logging.warning(f\"\\t{year}: No table found on the page.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"{year}: Failed due to {e}\")\n",
    "\n",
    "    sleep(0.5) # Politeness to the server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d54dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Verifying all years...\")\n",
    "# Check if all tables are present\n",
    "if len(all_tables) != len(YEARS):\n",
    "    missing_years = set(YEARS) - set(all_tables.keys())\n",
    "    logging.warning(f\"Missing years: {missing_years}\")\n",
    "\n",
    "logging.info(\"Comparing columns of all tables...\")\n",
    "first_table_columns = list(next(iter(all_tables.values())).columns)\n",
    "\n",
    "# Check for column differences\n",
    "for year, table in all_tables.items():\n",
    "    current_columns = list(table.columns)\n",
    "    if current_columns != first_table_columns:\n",
    "        missing_cols = set(first_table_columns) - set(current_columns)\n",
    "        exceeding_cols = set(current_columns) - set(first_table_columns)\n",
    "        if missing_cols:\n",
    "            logging.warning(f\"\\t{year}: Missing columns {missing_cols}\")\n",
    "        if exceeding_cols:\n",
    "            logging.warning(f\"\\t{year}: Exceeding columns {exceeding_cols}\")\n",
    "\n",
    "logging.info(\"Comparing columns of all renamed tables...\")\n",
    "# Rename and verify columns\n",
    "all_tables_renamed = {}\n",
    "for year, table in all_tables.items():\n",
    "    # Rename columns\n",
    "    all_tables_renamed[year] = table.rename(columns=RENAME_COLUMNS)\n",
    "    # Verify renaming\n",
    "    current_columns = list(all_tables_renamed[year].columns)\n",
    "\n",
    "    missing_cols =  set(ALL_COLUMNS) - set(current_columns)\n",
    "    exceeding_cols = set(current_columns) - set(ALL_COLUMNS)\n",
    "\n",
    "    if missing_cols:\n",
    "        logging.warning(f\"\\t{year}: Missing columns: {missing_cols}\")\n",
    "    if exceeding_cols:\n",
    "        logging.warning(f\"\\t{year}: Exceeding columns: {exceeding_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7296df21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all tables into a single DataFrame\n",
    "combined_df = pd.concat(all_tables_renamed.values(), keys=all_tables_renamed.keys(), names=[\"Year\", \"Row\"])\n",
    "\n",
    "# Drop irrelevant columns\n",
    "logging.info(f\"Dropping irrelevant columns: {IRRELEVANT_COLUMNS}\")\n",
    "combined_df.drop(columns=IRRELEVANT_COLUMNS, inplace=True)\n",
    "\n",
    "# Log the length of the DataFrame\n",
    "logging.info(f\"Combined dataset length: {len(combined_df)} rows\")\n",
    "\n",
    "# Log the number of empty values per column\n",
    "logging.info(\"Logging the number of empty values per column:\")\n",
    "for column in combined_df.columns:\n",
    "    empty_count = combined_df[column].isna().sum()\n",
    "    logging.info(f\"\\tColumn '{column}': {empty_count} empty values\")\n",
    "\n",
    "# Save the combined dataset to a CSV file\n",
    "logging.info(\"Saving combined dataset to CSV...\")\n",
    "combined_df.to_csv(\"../data/leopoldina_dataset_1885_1950.csv\", index=True, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d16298",
   "metadata": {},
   "source": [
    "# Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df36ea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inpute missing values\n",
    "df = pd.read_csv(\"../data/leopoldina_dataset_1885_1950.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b37659b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create json for missing data\n",
    "missing_data = {}\n",
    "for column in SHOULD_BE_NOTEMPTY_COLS:\n",
    "    for name in list(df[df[column].isna()][\"Name\"]):\n",
    "        if name not in missing_data:\n",
    "            missing_data[name] = {}\n",
    "        missing_data[name].update({column: \"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50af7471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the missing_data dictionary to a JSON file\n",
    "with open(\"../data/missing_data.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(missing_data, json_file, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrape_leopoldina",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
